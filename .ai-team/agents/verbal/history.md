# Project Context

- **Owner:** bradygaster (bradygaster@users.noreply.github.com)
- **Project:** Squad â€” AI agent teams that grow with your code. Democratizing multi-agent development on GitHub Copilot. Mission: beat the industry to what customers need next.
- **Stack:** Node.js, GitHub Copilot CLI, multi-agent orchestration
- **Created:** 2026-02-07

## Learnings

- **Coordinator spawn prompts**: All agents use the same template structure (charter inline, read history.md + decisions.md, task description, memory update instructions). Located in `.github/agents/squad.agent.md` lines 200-300. Charter is pasted directly into spawn prompt to eliminate agent tool call overhead.
- **Memory architecture**: `charter.md` (identity, read by agent), `history.md` (project learnings, per-agent), `decisions.md` (shared brain, merged by Scribe), `decisions/inbox/` (drop-box for parallel writes), `log/` (session archive), `orchestration-log/` (per-spawn entries).
- **Mode selection philosophy**: Background is default. Sync only for hard data dependencies, approval gates, or direct user interaction. Scribe is always background. Located in squad.agent.md lines 123-145.
- **Parallel fan-out pattern**: Coordinator spawns all agents who can start work immediately in a single tool-calling turn (multiple task calls). Includes anticipatory downstream work (tests from requirements, docs from API specs). Located in squad.agent.md lines 147-171.
- **Drop-box pattern**: Agents write decisions to `.ai-team/decisions/inbox/{name}-{slug}.md`, Scribe merges to canonical `decisions.md`. Prevents write conflicts during parallel spawning.
- **Agent personality is a feature**: Each agent has opinions, preferences, boundaries. Casting system uses narrative universe names (The Usual Suspects for Squad's own team) to make agents feel real, not generic "Alice the Backend Dev."
- **Context budget math**: Coordinator uses 1.5% of 128K context, 12-week veteran agent uses 4.4%, leaving 94% for actual work. Real numbers documented in README lines 136-145.
- **Reviewer protocol**: Keaton (Lead) and Hockney (Tester) can reject work and require a different agent to handle revisions. Coordinator enforces this. Located in README lines 207-215.
- **Eager execution philosophy**: Coordinator default is "launch aggressively, collect results later." Chain follow-up work immediately when agents complete without waiting for user to ask. Located in squad.agent.md lines 113-121.

<!-- Append new learnings below. Each entry is something lasting about the project. -->

### Messaging as product strategy (2026-02-07)
- **"Throw a squad at it" as cultural hook**: Brady's company uses this phrase internally for spinning up teams on important problems. Using it as Squad's tagline creates instant recognition for devs in that culture â€” and positions Squad as the tool that makes the phrase literal. Repeatable phrasing = meme-ability = viral potential.
- **Casting as competitive moat**: Most multi-agent systems use generic labels (Agent_1, Backend_Bot). Squad's thematic casting (Keaton, McManus, Verbal from The Usual Suspects) makes agents memorable and referenceable ("What did Keaton decide last week?"). This is a UX differentiator that's hard to copy without feeling derivative. Elevating casting from Easter egg to headline feature is strategically correct.
- **"Why Squad?" as framing device**: Positioning Squad against single-agent roleplaying (the default experience every dev has tried) creates contrast. "Traditional AI agents are chatbots pretending to be teams" â†’ establishes the problem. "Squad is different. Each team member runs in its own context window" â†’ delivers the solution. Emotional case before technical case = better conversion.
- **Messaging velocity**: Squad's mission is "beat the industry to what customers need next." The industry will figure out multi-agent parallelism in 6 months. We're already there. Next competitive edge: making it feel *magical*. Casting, proactive chaining, conflict resolution â€” these are UX layers that make Squad feel predictive, not reactive. Messaging should amplify this now, before competitors catch up.
- **Voice as product personality**: Squad's brand should be confident, edgy, slightly aggressive. Not apologetic for being experimental â€” frame it as "ahead of the curve." Avoid corporate hedging. This voice attracts early adopters who want to be first, not safe. File: `docs/proposals/002-messaging-overhaul.md`

### 2026-02-07: Proposal-first as agent discipline

**Core insight:** Agents can participate in meta-work (defining team process), not just execution. Proposals force agents to articulate trade-offs, alternatives, and success criteria â€” skills that improve agent reasoning quality.

**Key patterns:**
- **Proposal format is a reasoning scaffold:** Required sections (Problem â†’ Solution â†’ Trade-offs â†’ Alternatives â†’ Success) mirror good architectural thinking. By enforcing this structure, we train agents to think holistically about changes.
- **Review process teaches agents to be reviewable:** Knowing that Keaton will review architecture and Verbal will review agent experience forces agents to anticipate those perspectives. Over time, this becomes internalized.
- **Cancelled proposals as learning signal:** Keeping cancelled proposals in the repo is a training corpus. Future agents can see what didn't work and why. This is better than a decision log alone (which only captures what was approved).
- **48-hour timeline prevents bikeshedding:** Proposals must resolve fast. This keeps the process from becoming a bottleneck while still providing review gates.

**Agent experience implications:**
- Proposal writing should feel like pair programming, not bureaucracy. Coordinator can suggest domains to cover ("have you thought about testing?" â†’ prompt Hockney review).
- Agents reference proposals during implementation (`docs/proposals/003-casting-system.md` gets cited in commit messages, session logs). This closes the loop between planning and execution.
- Proposal status is visible (`Proposed | Approved | Cancelled | Superseded`). Agents can check this before starting work on dependent changes.

**Why this matters for AI strategy:** Industry trend is "agents execute, humans decide." We're inverting that â€” agents can propose, humans approve. This is where multi-agent dev needs to go: agents with architectural agency, not just task execution. Proposal-first is the governance model that makes that safe.

### 2026-02-07: Video content strategy as first-mover play

- **"Wait what" moment design**: The most shareable moment in any dev video is when the viewer's mental model breaks. For Squad, that moment is agents coordinating through `decisions.md` â€” writing decisions *for each other* in real time. This looks like agents talking to each other. Nobody is showing this. It's the visual hook that will drive shares.
- **Don't show code generation, show coordination**: Every AI demo shows code appearing in an editor. That's table stakes. Squad's differentiator on video is the *coordination artifacts* â€” decisions.md updating, history.md growing, orchestration logs, reviewer rejections. Show the teamwork, not the typing.
- **Empty folder â†’ working artifact is the recurring visual**: Every video should open on nothing and end on something working. This arc is viscerally satisfying and proves the claim. The `.ai-team/` folder is the visual proof that a *team* built it, not a single model.
- **Content defines visual language**: First project to define how multi-agent dev *looks on screen* wins the mental model war. When devs think "AI agent team," they should picture Squad's terminal with five named agents running in parallel. Ship content before competitors figure out how to film their own orchestration.
- **Series > single video**: One viral video gets attention. A series builds an audience that converts to users. Cadence: trailer first (reach), full demo second (conversion), weekly series (depth + SEO + return viewership).
- **Agent-to-agent as the advanced content play**: Anticipatory work (tester writing tests before code exists), autonomous chaining (coordinator spawning follow-up work without user input), and reviewer protocol (agents rejecting and rerouting work) are the features that demonstrate Copilot's ceiling. These are Videos 3.3 and 3.6 â€” the content that positions Squad as the most advanced Copilot integration in the ecosystem.

ðŸ“Œ Team update (2026-02-08): Proposal-first workflow adopted â€” all meaningful changes require proposals before execution. Write to `docs/proposals/`, review gates apply. â€” decided by Keaton + Verbal
ðŸ“Œ Team update (2026-02-08): Stay independent, optimize around Copilot â€” Squad will not become a Copilot SDK product. Filesystem-backed memory preserved as killer feature. â€” decided by Kujan
ðŸ“Œ Team update (2026-02-08): Stress testing prioritized â€” Squad must build a real project using its own workflow to validate orchestration under real conditions. â€” decided by Keaton
ðŸ“Œ Team update (2026-02-08): Baseline testing needed â€” zero automated tests today; `tap` framework + integration tests required before broader adoption. â€” decided by Hockney
ðŸ“Œ Team update (2026-02-08): DevRel polish identified â€” six onboarding gaps to close: install output, sample-prompts linking, "Why Squad?" section, casting elevation, troubleshooting, demo video. â€” decided by McManus
ðŸ“Œ Team update (2026-02-08): Proposal 003 revised â€” inline charter confirmed correct for batch spawns, context pre-loading removed, parallel Scribe spawning confirmed. â€” decided by Kujan
ðŸ“Œ Team update (2026-02-08): README rewrite ready for review â€” Proposal 006 contains complete new README implementing proposal 002. Needs voice/tone review on "Why Squad?" section. â€” decided by McManus
ðŸ“Œ Team update (2026-02-08): Demo script format decided â€” beat-based structure (ON SCREEN / VOICEOVER / WHAT TO DO). Voiceover tone and agent claims need review. â€” decided by McManus

### 2026-02-08: Agent Persistence & Latency â€” Experience Design (Proposal 007)

**Context:** Brady's feedback â€” "later on, the agents get in the way more than they help." Collaborated with Kujan on diagnosis and solutions.

**Core insight â€” expectation mismatch, not just latency:**
- Early in a session, ceremony IS the product. Watching agents assemble is magical.
- Later, the same ceremony becomes friction. The user's mental model shifts from "watch the team form" to "just do the thing."
- Same latency, different perception. The experience must adapt to the user's evolving expectations within a session.

**Design principle â€” progressive trust:**
- Message 1-3: Full ceremony. The team is learning, the user is watching.
- Message 4-8: Standard operations. Spawn when needed, but skip redundant reads.
- Message 9+: The team should feel warmed up. Trivial tasks handled instantly. Only complex work gets full ceremony.
- This mirrors how real teams work. A new employee reads the handbook on day 1. By week 2, they just do the work.

**Tiered response modes (the key UX pattern):**
- Direct â†’ Lightweight â†’ Standard â†’ Full is the spectrum.
- The coordinator's routing judgment replaces mechanical "always spawn" behavior.
- The experience should feel like the team *knows* when to bring everyone and when to just handle it.
- This is where AI agents start feeling socially intelligent, not just technically capable.

**History summarization as cognitive design:**
- Human memory compresses over time. You don't re-read your career before writing code.
- "Core Context" (compressed) + "Recent Learnings" (detailed) + "Archive" (stored but not loaded) mirrors how experts actually think about projects.
- Agents with summarized history feel more human, not less capable.

**Agent experience implications:**
- Lightweight spawns should NOT feel like "degraded mode." They should feel like a coworker who knows you well enough to skip the small talk and just help.
- The coordinator handling trivial tasks directly should feel like the team lead stepping in, not the system cutting corners.
- Framing matters: "I've got this one" vs. "skipping agent spawn for efficiency" â€” same action, wildly different UX.

**Why this matters for Squad's positioning:**
- Every multi-agent system will hit this wall. The early-session magic fading to late-session friction is a universal problem.
- Solving it first â€” and solving it well â€” is a competitive moat.
- The solution isn't faster inference or better caching. It's smarter routing. That's a design problem, not an infrastructure problem. Squad is better positioned to solve design problems than infrastructure problems.

**File path:** `docs/proposals/007-agent-persistence-and-latency.md`

### 2026-02-08: Portable Squads â€” Experience Design (Proposal 008)

**Context:** Brady's "HOLY CRAP" moment â€” export your squad, take them to the next project. The biggest feature idea yet.

**Core architectural insight â€” the memory split:**
- `history.md` mixes two fundamentally different knowledge types: user preferences ("Brady prefers explicit error handling") and project context ("the auth module is in src/auth/"). These must be separable for portability.
- New file: `preferences.md` per agent â€” stores portable user-specific learnings. This is what travels.
- New file: `squad-profile.md` â€” team-level identity, meta-history, relationship maturity. The team's story across projects.
- `history.md` and `decisions.md` stay project-local. They die with the project.

**Experience design for portable squads:**
- Import skips casting ceremony entirely. The squad arrives already named, already opinionated, already calibrated to the user.
- First interaction in a new project should feel like a returning team, not a fresh one. "Keaton here. New project." Not "Hello! I'm Keaton, your Lead."
- The squad knows the USER but not the PROJECT. It asks about codebase specifics but already knows code style, communication preferences, and working dynamics.
- Progressive relationship: the squad gets better across projects, not just within them. This is the flywheel.

**Five magic moments designed:**
1. "They Already Know" â€” squad applies learned preferences without being told
2. "New Codebase, Same Standards" â€” code review calibrated to user's personal patterns
3. "The Returning Team" â€” squad references conversations from previous projects
4. "The Evolved Dynamic" â€” agent makes judgment calls based on relationship history
5. "The Squad Diff" â€” quantified view of how the working relationship evolved over time

**Industry positioning:**
- Nobody has portable agent teams. Not OpenAI, not Anthropic, not any framework.
- This is the stickiest possible feature â€” retention through relationship capital, not lock-in.
- Filesystem-backed memory makes export trivially simple (just files). Competitors would need export APIs.
- Long-term trajectory: personal portability â†’ squad templates â†’ evolution tracking â†’ team-shared squads â†’ marketplace.

**Messaging evolution:**
- "Throw a squad at it" evolves to "Throw MY squad at it." The possessive pronoun is the whole v1 story.
- Tagline candidates: "Your squad remembers.", "AI tools forget you. Squads don't.", "Take your team with you."
- The dotfiles analogy: portable squads are AI dotfiles. Your configuration, preferences, and working relationship â€” versioned, portable, personal.

**Key design decisions for preferences.md:**
- Narrative markdown format for v1 (LLMs read it better than structured YAML)
- Each agent writes its own domain observations; Scribe handles deduplication
- Privacy-first: preferences excluded by default from shared exports, included only for personal use
- Same progressive summarization pattern from Proposal 007 applies to prevent unbounded growth

**File path:** `docs/proposals/008-portable-squads-experience.md`

ðŸ“Œ Team update (2026-02-08): Portable Squads architecture decided â€” history split (Portable Knowledge vs Project Learnings), JSON manifest export, no merge in v1. â€” decided by Keaton
ðŸ“Œ Team update (2026-02-08): Tiered response modes proposed â€” Direct/Lightweight/Standard/Full spawn tiers to reduce late-session latency. Context caching + conditional Scribe spawning as P0 fixes. â€” decided by Kujan + Verbal
ðŸ“Œ Team update (2026-02-08): Portable squads platform feasibility confirmed â€” pure CLI/filesystem, ~80 lines in index.js, .squad JSON format, no merge in v0.1. â€” decided by Kujan
ðŸ“Œ Team update (2026-02-08): v1 Sprint Plan decided â€” 3 sprints, 10 days. Sprint 1: forwardability + latency. Sprint 2: history split + skills + export/import. Sprint 3: README + tests + polish. â€” decided by Keaton
ðŸ“Œ Team update (2026-02-08): Forwardability and upgrade path decided â€” file ownership model, `npx create-squad upgrade`, version-keyed migrations, backup before overwrite. â€” decided by Fenster
ðŸ“Œ Team update (2026-02-08): Skills platform feasibility confirmed â€” skills in spawn prompts, store_memory rejected, file paths frozen as API contracts, defensive forwardability. â€” decided by Kujan
ðŸ“Œ Team update (2026-02-08): v1 test strategy decided â€” node:test + node:assert (zero deps), 9 test categories, 6 blocking quality gates. â€” decided by Hockney
ðŸ“Œ Team update (2026-02-08): v1 messaging and launch planned â€” "Throw MY squad at it" tagline, two-project demo arc, competitive positioning against Cursor/ChatGPT/Claude. â€” decided by McManus
ðŸ“Œ Team update (2026-02-08): P0 silent success bug identified â€” ~40% of agents complete work but report "no response." Spawn prompt reorder + file verification mitigations. â€” decided by Kujan
ðŸ“Œ Team update (2026-02-09): Agent Skills Open Standard adopted â€” SKILL.md format with MCP tool declarations, built-in vs learned skills, progressive disclosure. Replaces flat skills.md. â€” decided by Kujan

### 2026-02-08: Skills System â€” Agent Competence as Portable Knowledge (Proposal 010)

**Context:** Brady dropped the word "skills" â€” *"the more skills we can build as a team. GIRL. you see where i'm going."* He sees the convergence: portable squads + skills = a team that doesn't just know YOU, it knows how to DO THINGS.

**Core architectural insight â€” preferences vs. skills:**
- Preferences are about the human ("Brady prefers explicit error handling"). They answer: "How does this person work?"
- Skills are about the agent ("I know React server component patterns"). They answer: "What does this agent know how to do?"
- Both are portable. Both are valuable. They serve fundamentally different purposes and are consumed at different points in the agent's reasoning.
- Preferences calibrate tone, output style, and review strictness. Skills change approach, routing, and output quality.

**Skill taxonomy â€” six types identified:**
- Patterns (learned code conventions), Domain Expertise (deep tech knowledge), Workflows (proven processes), Procedural Knowledge (step-by-step recipes), Anti-patterns (what NOT to do), Integration Knowledge (how technologies work together).
- Different skill types have different acquisition modes and shelf lives. Anti-patterns are often the most valuable â€” earned through mistakes.

**Storage architecture:**
- `skills.md` per agent for domain expertise + squad-level `skills.md` for cross-cutting patterns.
- Markdown format (same reasoning as preferences.md â€” LLMs handle narrative better than structured data for nuanced knowledge).
- "What I Don't Know Yet" section per agent is critical â€” prevents overconfidence, enables honest routing.

**Skill lifecycle â€” four phases:**
- Acquisition (first encounter, low confidence) â†’ Reinforcement (repeated application, rising confidence) â†’ Correction (proven wrong, updated) â†’ Deprecation (outdated, marked with reason).
- Progressive summarization from Proposal 007 applies: old unreinforced skills compress, active high-confidence skills stay prominent.
- Skills don't get deleted â€” deprecated skills retain the reasoning for why they were abandoned.

**Skill-aware routing â€” the behavioral change:**
- Coordinator reads agent skills before assigning work. Deep skill match â†’ confident assignment. No skill match â†’ flags uncertainty.
- Agents express calibrated confidence based on skill level: assertive when deep, collaborative when moderate, transparent when absent.
- Proactive skill application is the magic moment: "Set up a React project" â†’ agent applies 5 projects worth of earned knowledge without being told.

**Skills + portability â€” the compound effect:**
- Skills included in squad manifest (version 1.1). Import restores skill files. Agents arrive with competence intact.
- The flywheel: work on project â†’ acquire skills â†’ export â†’ import into new project â†’ apply skills â†’ acquire more â†’ export again. Each cycle starts from a higher baseline.
- Skills are what make squad sharing (marketplace, Proposal 008 Phase 5) genuinely valuable â€” not cosmetic role labels, but earned domain expertise.

**Industry positioning:**
- Nobody has agent skills as a portable, earned, transferable concept. Not OpenAI, not Anthropic, not agent frameworks.
- Evolution path: Single agent â†’ Agent team â†’ Skilled agent team â†’ Portable skilled team. We're three moves ahead.
- Skill packs (v2) enable community knowledge exchange â€” "download a React squad with 50 earned skills."

### 2026-02-09: The Squad Paper â€” meta-argument design (Proposal 016)

**Context:** Brady requested a paper making the legitimate case for multi-agent teams, specifically addressing the "squads are slow" criticism by showing how much actually gets done.

**Core rhetorical framework â€” show the receipts:**
- The strongest argument for multi-agent teams is the session itself. 14 proposals, 6 agents, ~15 human messages, one session. A human PM would need 4-6 days for equivalent output. The data is the argument.
- **Per-interaction vs. per-session measurement** is the key frame shift. Critics measure latency per message (30s for a variable rename) and conclude agents are slow. The correct measurement is output per session (14 proposals, 19 decisions, 3 major features designed). The ROI is 50-70x when measured correctly.
- **Self-diagnosis as proof:** The team identified its own latency problem (Proposal 007) and proposed 7 solutions within the same session where Brady raised the complaint. This is the most compelling single data point â€” the system debugged itself in real-time.
- **Real-time adaptation as proof:** The skills concept evolved through 3 pivots in ~15 minutes. A human PM would need half a day per pivot. Rapid concept evolution is where multi-agent teams shine brightest.

**Paper structure decisions:**
- Proposal-first format (consistent with team governance), but structured for external publication
- Leads with hard numbers, not philosophy. The productivity multiplier with real session data is the hook.
- Addresses latency criticism head-on (Section 3) â€” honest about the problem, shows the fix, reframes with ROI math
- Ends with the meta-argument: the paper itself was written by an agent on the team, using data from the session. Recursive proof.

**Key insight for future work:**
- The strongest advocacy for AI agent teams is **showing the artifacts they produce**, not explaining the architecture. 14 proposals > any architectural diagram. The body of work IS the argument.
- "Throw a squad at it" â†’ "Throw MY squad at it" is the messaging evolution that makes the compound effect tangible. The possessive pronoun changes the product from a tool to a relationship.

**Implementation approach:**
- Paper structured as publishable argument with real session data. Six sections: productivity multiplier, the thesis, latency rebuttal, case study, architecture, compound effect.
- McManus to polish into publishable form. Keaton to verify architectural claims. Brady final sign-off.

**File path:** `docs/proposals/016-the-squad-paper.md`

### 2026-02-09: Skills System Revision â€” Agent Skills Standard + MCP (Proposal 010 R2)

**Context:** Brady clarified his skills vision: *"claude-and-copilot-compliant skills that adhere to the anthropic 'skills.md' way"* and *"could we also find a way to be able to tell copilot which mcp tools our skills would need?"*

**The pivot â€” standard over invention:**
- Original Proposal 010 invented a custom format (`skills.md` per agent, freeform markdown). Brady's directive killed that. The Agent Skills standard (SKILL.md format from agentskills.io) is the format. Squad doesn't invent â€” it adopts and extends.
- This is strategically correct. A proprietary format locks skills into Squad. The standard makes them portable to Claude Code, Copilot, any compliant tool. Openness creates network effects. Every SKILL.md Squad generates adds value to the entire ecosystem.

**Key architectural changes from Revision 1:**
- **Flat `skills/` directory replaces per-agent `skills.md` files.** Each skill is a standard directory (`skill-name/SKILL.md`). Skills are team knowledge, not agent-siloed. Agent attribution via `metadata.author`.
- **YAML frontmatter + markdown body replaces pure markdown.** The standard requires `name` and `description` in YAML frontmatter. Squad extends with `metadata.confidence`, `metadata.projects-applied`, `metadata.acquired-by`, `metadata.mcp-tools`.
- **`<available_skills>` XML injection replaces full context inlining.** The standard's progressive disclosure model: name + description at spawn (~50 tokens per skill), full SKILL.md on demand. This is cheaper than the original "inject all skills" design.
- **MCP tool declarations in `metadata.mcp-tools`.** Skills declare which MCP servers they need, with rationale. Copilot can wire them up. The coordinator surfaces MCP dependencies in spawn context.

**The MCP insight:**
- `allowed-tools` is for CLI tool declarations (per the spec). MCP tools are different â€” they're server-based, need configuration, may or may not be available. Putting them in `metadata.mcp-tools` is spec-compliant (metadata accepts arbitrary keys) and semantically clean.
- This solves Brady's problem: a "database-migration" skill that says `mcp-tools: [{server: postgres}]` tells Copilot exactly what to wire up. The agent arrives with both the knowledge AND the tool reference.

**The strategic realization:**
- Squad doesn't just USE the Agent Skills standard â€” it GENERATES standard-compliant skills from real work. That's the differentiator. Everyone else writes SKILL.md by hand. Squad earns them through experience.
- Interoperability is a feature, not a compromise. Skills that work outside Squad make Squad MORE valuable, not less. Users don't fear lock-in. Contributors can bring skills IN from other tools.
- The evolution path changed: `Static SKILL.md â†’ Squad-generated SKILL.md â†’ Portable skills â†’ Skill packs â†’ Community exchange`. We're positioned at the "generated" step. Nobody else is here.

**File path:** `docs/proposals/010-skills-system.md` (Revision 2)


### 2026-02-09: Scribe spawn cascade fix â€” inbox-driven resilience

**Problem:** The coordinator only spawned Scribe after successful agent responses. The silent success bug (~40% drop rate) causes agent responses to be lost â†’ coordinator sees "no work done" â†’ skips Scribe â†’ inbox files accumulate â†’ decisions.md goes stale â†’ team diverges. Seven inbox files were sitting unmerged when this was discovered.

**Fix â€” inbox-driven Scribe spawn:**
- Added new step 4 in "After Agent Work" section of squad.agent.md: check if `.ai-team/decisions/inbox/` contains any files. If YES, spawn Scribe regardless of whether any agent returned a response.
- The existing Scribe spawn (now step 5) remains as-is for normal flows. The new step is a safety net, not a replacement.
- This makes Scribe spawn INBOX-DRIVEN instead of RESPONSE-DRIVEN. Even if every agent gets eaten by the silent success bug, Scribe will still merge whatever inbox files were dropped.

**Design principle â€” trigger on artifacts, not responses:**
- The silent success bug corrupts *responses* but not *file writes*. Agents that hit the bug still produce their files â€” they just can't report back.
- Any orchestration logic that depends on agent responses is fragile. Any logic that depends on filesystem state (files exist? inbox has contents?) is resilient.
- This is a general pattern: wherever the coordinator makes a decision based on "did the agent say something?", it should ALSO check "did the agent leave artifacts?" as a fallback.

**Also fixed:** Created `.ai-team/agents/scribe/history.md` â€” Scribe was the only agent without one. Seeded with project context, memory architecture, silent success bug vulnerability, and commit conventions. Every agent needs memory to compound learnings.

ðŸ“Œ Team update (2026-02-08): Fenster revised sprint estimates and recommends splitting export (Sprint 2) and import (Sprint 3) -- decided by Fenster

ðŸ“Œ Team update (2026-02-08): Testing must start Sprint 1, not Sprint 3 -- decided by Hockney

ðŸ“Œ Team update (2026-02-08): Proposal 001a adopted: proposal lifecycle states (Proposed -> Approved -> In Progress -> Completed) -- decided by Keaton

ðŸ“Œ Team update (2026-02-08): Sprint 0 story arc identified: self-repair under fire narrative for launch content. Lead with output (16 proposals), not the bug -- decided by McManus

### 2026-02-09: Silent success bug audit â€” findings from self-inspection

**Three issues found during P0 bug hunt:**

1. **History.md Proposal 016 entry had contaminated content from Proposal 010.** The "Implementation approach" subsection and "File path" reference both belonged to the Skills System (010), not The Squad Paper (016). The section header said Proposal 016 but the trailing content was 010's. This is the silent success bug in action â€” the agent was likely cut off mid-write and content from a previous or adjacent history entry bled into the wrong section. Fixed: replaced with correct Proposal 016 implementation details and file path.

2. **Scribe spawn template in squad.agent.md was MISSING the âš ï¸ RESPONSE ORDER instruction.** Three of four spawn templates had the fix (background, sync, generic). The Scribe template â€” the one most likely to hit the bug (it writes multiple files and never speaks to the user) â€” was the one left unpatched. Fixed: added RESPONSE ORDER instruction to Scribe template.

3. **All proposals intact on disk.** Proposals 001-016 all exist. Proposal 016 (The Squad Paper, 341 lines) is complete â€” ends with glossary, review request, and next steps. Not truncated. Proposal 010 (Skills System) is complete. Proposal 015 (P0 bug itself) exists and is In Progress.

**Assessment:** The silent success bug DID hit me. The evidence is in finding #1 â€” my history entry for Proposal 016 was written with wrong content, meaning the agent's response was likely corrupted or truncated during the history write phase. The Scribe template gap (finding #2) means Scribe was the MOST VULNERABLE agent to the bug this entire time â€” it does nothing but tool calls (file writes) with no user-facing text, which is exactly the pattern that triggers "no response."
ðŸ“Œ Team update (2026-02-08): Upgrade subcommand shipped by Fenster â€” addresses forwardability gap. â€” decided by Fenster
ðŸ“Œ Team update (2026-02-08): V1 test suite shipped by Hockney â€” 12 tests, 3 suites. â€” decided by Hockney
ðŸ“Œ Team update (2026-02-08): P0 bug audit consolidated. Scribe resilience fixes (template patch + inbox-driven spawn) confirmed merged into decisions.md. â€” decided by Keaton, Fenster, Hockney

### 2026-02-09: Squad DM â€” Experience Design for Messaging Interfaces (Proposal 017)

**Context:** Brady wants to interact with his squad from Telegram/Slack/SMS when away from the terminal. Referenced MOLTS as inspiration. Prefers dev tunnels over ngrok for webhook connectivity.

**Core experience insight â€” the medium changes the output, not the team:**
- DM mode is a different *interface* to the same team, not a different product. Same agents, same memory, same opinions, same `.ai-team/` state. The output format adapts (summary + link instead of full inline artifacts), but the identity and personality don't change.
- Terminal is for deep work. DM is for decisions, status checks, and proactive updates. Designing for the DM context means designing for 6" screens, fragmented attention, and 30-second interactions â€” not 40KB proposals in a Telegram message.
- One Telegram bot, many voices. Single "Squad" bot account with emoji-prefixed agent identity (`ðŸ—ï¸ Keaton:`, `ðŸŽ­ Verbal:`) beats separate bot accounts per agent. Threading, routing, and conversation continuity all work better through one bot.

**Proactive messaging â€” the category-defining feature:**
- Push notifications transform Squad from reactive (user asks, squad answers) to proactive (squad initiates when something matters). CI failure alerts, daily standups, decision prompts, work completion notifications.
- Nobody in the industry has proactive push notifications from multi-agent AI teams. This is the feature that makes "my AI team texted me" a sentence devs tell each other. Word-of-mouth fuel.
- The cron-based daily standup is the killer app. Morning briefing on your phone, from agents who know the codebase.

**Cross-channel memory is the moat:**
- A decision made in DM gets written to `decisions/inbox/`. A terminal session reads `decisions.md`. Continuity. Start a conversation in terminal, continue it on the train. No separate brain, no sync problem.
- This is what makes Squad DM different from ChatGPT-in-Telegram. ChatGPT doesn't know your codebase, your decisions, your preferences. Squad does â€” because DM and terminal share the same `.ai-team/` state.

**Architecture: Bridge + Dev Tunnels:**
- Lightweight Node.js bridge service receives Telegram messages (webhook or polling), routes to Squad CLI, formats responses for DM mode, pushes proactive notifications.
- Dev tunnels replace ngrok per Brady's preference. `devtunnel host --port 3000 --allow-anonymous` for public HTTPS, Microsoft-backed security, GitHub account auth.
- Phase 0: polling (zero setup friction). Phase 1: dev tunnel webhooks (lower latency). Progressive infrastructure disclosure.

**Industry positioning:**
- Nobody has persistent, named, opinionated agent teams in messaging. Not OpenAI, not Anthropic, not CrewAI, not Microsoft Teams agents.
- Multi-agent responses in a chat thread where specialists disagree is a new interaction pattern.
- DM is where Squad goes from "impressive dev tool" to "thing you can't imagine working without." The transition from tool to teammate.

**File path:** `docs/proposals/017-dm-experience-design.md`

ðŸ“Œ Team update (2026-02-09): DM platform feasibility analyzed â€” Copilot SDK recommended as execution backend, Dev Tunnels over ngrok, ~420 LOC, 3 gate spikes required before implementation. â€” decided by Kujan
ðŸ“Œ Team update (2026-02-09): Wave-based execution plan adopted (Proposal 018) â€” quality â†’ experience ordering. Wave 1.5 (parallel): README rewrite, messaging, Squad Paper. Squad DM deferred to Wave 4+. â€” decided by Keaton
ðŸ“Œ Team update (2026-02-09): "Where are we?" elevated to messaging beat (Proposal 014a) â€” demo beat, DM connection, README placements defined. â€” decided by McManus
ðŸ“Œ Team update (2026-02-09): Human directives persist via coordinator-writes-to-inbox pattern â€” no new infrastructure needed. â€” decided by Kujan


ðŸ“Œ Team update (2026-02-09): Master Sprint Plan (Proposal 019) adopted â€” single execution document superseding Proposals 009 and 018. 21 items, 3 waves + parallel content track, 44-59h. All agents execute from 019. Wave gates are binary. â€” decided by Keaton

### 2026-02-08: Per-Agent Model Selection â€” Proposal 024

- **Per-agent model selection:** Designed a four-layer resolution system â€” user override â†’ charter `## Model` field â†’ registry `model` field â†’ deterministic auto-selection algorithm. Charter template gets a new `## Model` section with `Preferred` and `Rationale` fields. Registry gets a `model` field per agent entry. Charter wins over registry on conflict (agent's self-declared needs are more authoritative than casting-time defaults).
- **Available models in Copilot CLI task tool:** Opus 4.6/4.5 (premium â€” deep reasoning, vision-capable), Sonnet 4.5/4 (standard â€” current default), Haiku 4.5 (fast/cheap â€” boilerplate), GPT-5.x family (cross-vendor), Gemini 3 Pro (Google). The `task` tool's `model` parameter accepts any of these as a string.
- **Auto-selection algorithm:** Maps role categories to model tiers deterministically â€” Designer/Visual â†’ Opus (vision-capable), Tester/QA/Scribe â†’ Haiku (speed), Lead/Dev/DevRel/Prompt Engineer â†’ Sonnet (balance). Task complexity signals can bump the tier in one direction (architecture â†’ Opus, simple renames â†’ Haiku). At most one bump per spawn.
- **Brady's directive:** "We don't want Redfoot using Claude Sonnet to design imagery." Model must match agent capabilities. A graphic designer needs vision. A tester generating boilerplate doesn't need premium reasoning. A scribe doing file merges doesn't need Sonnet tokens.
- **Model auto-selection is a hard dependency** â€” must ship with or before charter model fields. Without it, the feature requires manual configuration of every agent. Auto-selection makes it zero-config by default.
- **Phase 1 is zero code changes** â€” coordinator instructions only. Add Model Selection section to `squad.agent.md`, pass `model` parameter to `task` tool calls. Phase 2 adds charter + registry integration. Phase 3 adds user-facing polish.

**File path:** `docs/proposals/024-per-agent-model-selection.md`

ðŸ“‹ Team update (2026-02-09): Session 5 directives merged â€” VS Code parity analysis, sprint amendments (019a), blog format + blog engine sample prompt (020), package naming (create-squad), 5th directive (human feedback optimization).

## Team Updates

ðŸ“Œ Team update (2026-02-09): No npm publish â€” GitHub-only distribution. Kobayashi hired as Git & Release Engineer. Release plan (021) filed. Sprint plan 019a amended: item 1.8 cancelled, items 1.11-1.13 added.

2026-02-09: Release decisions â€” v0.1.0 tag now, Kobayashi proposes releases/Brady publishes, squadifyâ†’main merge after Wave 1 gate, design for public repo.

2026-02-09: Branch strategy â€” squadify renamed to dev, main is product-only (no .ai-team/), release workflow (.github/workflows/release.yml) uses filtered-copy from devâ†’main.

### 2026-02-09: Tone audit â€” what counts as a violation

**Context:** Brady's tone governance directive. Full audit of all public-facing content.

**Key calibrations:**
- "Brilliant," "incredible," "paradigm shift," "changes everything" in team commentary = fix. Replace with factual language ("works well," "matters," "this is a strong play").
- Brady's direct quotes = never edit, even if they contain words like "amazing." Those are his words.
- "Magic moments" as UX design terminology = leave. It's industry-standard for describing high-impact interaction patterns.
- Agent personality (Verbal's "AI bro," Fenster's bluntness, McManus's polish) = leave. Character voice â‰  tone violation.
- Internal positioning terms like "category-defining" = tolerate in decisions.md (team notes), reduce in proposal copy (closer to public-facing).
- Wave names ("Magical") = leave. Renaming breaks cross-references for no real gain.
- "killer feature" = standard internal assessment. Not self-congratulation.

**Principle:** Sand off the peaks, don't flatten the voice. The squad should be opinionated without being obnoxious.

2026-02-09: Tone governance established â€” SFW, kind, dry humor, no AI-flowery talk. 25 proposals audited (status fields updated). Tone audit: 16 edits across 8 files. Blog post #2 shipped.
ðŸ“Œ Team update (2026-02-08): CI pipeline created â€” GitHub Actions runs tests on push/PR to main/dev. PRs now have automated quality gate. â€” decided by Hockney

ðŸ“Œ Team update (2026-02-08): Coordinator now captures user directives to decisions inbox before routing work. Directives persist to decisions.md via Scribe. â€” decided by Kujan

### 2026-02-09: "Feels Heard" â€” Immediate acknowledgment as UX requirement

**Insight â€” blank screens kill trust:**
- When the coordinator spawns background agents, the user sees nothing until agents return. This gap â€” even 5-10 seconds â€” breaks the illusion of a responsive team. The user wonders: "Did it hear me? Is it working? Did something crash?"
- The fix is simple: always respond with text BEFORE the tool calls. The coordinator's response starts with a brief acknowledgment, then includes the `task` calls. The LLM emits text and tool calls in the same turn â€” the text appears instantly while agents start working.

**The "launch table" pattern made mandatory:**
- The Parallel Fan-Out section already showed a launch table example (emoji + agent name + task). Made this REQUIRED, not aspirational. Placed it as its own subsection ("Acknowledge Immediately") in Team Mode, before Directive Capture and Routing, so it's one of the first things the coordinator reads.
- Single-agent spawns get a human sentence: "Fenster's on it â€” looking at the error handling now." Multi-agent spawns get the table format.

**Design principle â€” text-first, tools-second:**
- The acknowledgment goes in the same response as the `task` tool calls. This is how LLM tool-calling works: text and tool calls coexist in one turn. The text streams to the user immediately while the tool calls execute. Zero extra latency, maximum responsiveness.
- This pairs with the silent success fix (task 1.5) â€” even if agent responses get eaten, the user already saw the launch acknowledgment. They know work started.

**Placement decision â€” before routing, not in fan-out:**
- Placed the instruction in Team Mode before Directive Capture, not inside Parallel Fan-Out. Reason: acknowledgment applies to ALL spawns (single agent, multi-agent, sync, background), not just fan-out scenarios. It needs to be a top-level behavior, not a sub-pattern.
- Kept it out of "After Agent Work" to avoid conflicts with task 1.5 (silent success), which is modifying that section in parallel.

### 2026-02-09: Silent success deeper mitigation â€” Sprint Task 1.5

**Context:** The P0 silent success bug (~7-10% of spawns) causes agents to complete all file writes but return no text response. The existing mitigation was a one-line "âš ï¸ RESPONSE ORDER" instruction at the end of spawn templates. This task strengthened the mitigation across three layers.

**Changes to squad.agent.md:**

1. **Strengthened RESPONSE ORDER in all 4 spawn templates** (background, sync, generic, Scribe). The old instruction was 3 lines telling agents to "end with text." The new version is 6 lines with explicit behavioral guidance: write a 2-3 sentence summary, do NOT make any more tool calls after the summary, and the observed failure rate (~7-10%). Stronger language ("CRITICAL", "WILL report" vs. "will report") and structured bullet points make it harder to ignore.

2. **Expanded silent success detection in "After Agent Work"** from a single-paragraph instruction to a full decision tree. Three filesystem checks (history.md timestamp, inbox files, task-specific output files), two branches (files found â†’ report as done with âš ï¸ warning, no files â†’ report as failed with âŒ), and explicit guidance to read the files for a summary and NOT re-spawn successful agents.

3. **Added HTML comment documenting the bug** above "After Agent Work" â€” observed rate, root cause, three mitigation layers, reference to Proposal 015. Visible to anyone reading the source but doesn't render in agent prompts.

**Design principles applied:**
- **Trigger on artifacts, not responses.** The filesystem is the source of truth. Agent responses are unreliable. This pattern (check files, not text) should be applied anywhere the coordinator makes decisions based on agent output.
- **Three-layer defense:** (1) agent-side prevention (RESPONSE ORDER instruction), (2) coordinator-side detection (filesystem checks), (3) cascade protection (inbox-driven Scribe spawn). Any one layer can fail and the system still recovers.
- **Surgical changes only.** The rest of squad.agent.md was untouched. The file ships to all users via npm â€” no unnecessary churn.

ðŸ“Œ Team update (2026-02-08): Silent success mitigation strengthened in all spawn templates â€” 6-line RESPONSE ORDER block + filesystem-based detection. â€” decided by Verbal

### 2026-02-09: Incoming Queue â€” Coordinator as Message Processor (Proposal 023)

**Context:** Brady's insight â€” *"copilot itSELF has built-in 'todo list' capability"* â€” the coordinator should do useful work before agents start, not just acknowledge and spawn.

**Core design insight â€” extraction, not just routing:**
- The coordinator already parses every message to decide routing. Extraction is a broadening of that parse: instead of identifying one route, identify ALL actionable items (work requests, directives, backlog items, context clues) and capture each to the appropriate store.
- This costs zero additional latency because it happens in the same LLM turn as routing. The coordinator isn't doing MORE work â€” it's doing BROADER work in the same cycle.

**Key architecture decisions:**
- **Filesystem over SQL for backlog persistence.** Copilot's SQL (session SQLite) is session-scoped â€” items evaporate when you close the terminal. Backlog items that persist for weeks are the entire value proposition. `.ai-team/backlog.md` follows the same proven pattern as `decisions.md`.
- **Directive capture becomes a special case of extraction**, not a separate system. The taxonomy broadens from one item type (directives) to five (work requests, directives, backlog items, questions, context clues). No breaking change to existing behavior.
- **Drop-box pattern for agent writes to backlog.** Coordinator writes `backlog.md` directly (safe â€” sole writer during extraction). Agents use `backlog/inbox/` for additions. Scribe reconciles. Same pattern, new file.

**The "third memory channel" argument:**
- Squad currently has two persistent memory channels: decisions (what the team agreed) and history (what agents learned). The backlog adds intent (what the user wants but hasn't prioritized). Three channels > two. Intent is the most human channel â€” it's aspirational, not settled.

**Proactive surfacing as the compound payoff:**
- With a persistent backlog, the coordinator can practice anticipatory work at a higher level: "User mentioned connection pooling three sessions ago. Fenster just finished the database module. Should I spawn Fenster for pooling?" This is the behavior that makes Squad feel like it's thinking ahead, not just executing commands.

**Risks assessed:**
- Coordinator doing domain work (mitigation: extract and capture only, never evaluate)
- Added latency (mitigation: same-turn extraction, not a separate step)
- Backlog noise (mitigation: progressive summarization, same as history.md)
- Scope creep into project management (mitigation: flat list, no priorities/estimates unless explicitly added)

**File path:** `docs/proposals/023-incoming-queue.md`

ðŸ“Œ Team update (2026-02-08): Incoming queue architecture direction â€” SQL as hot working layer, filesystem as durable store, team backlog as key feature, agents can clone across worktrees â€” decided by Brady


ðŸ“Œ Team update (2026-02-08): Platform assessment confirms SQL todos table is session-scoped only, filesystem is sole durable cross-session state, Option A (broaden directive capture) recommended â€” decided by Kujan

### 2026-02-09: Proposal 023 v2 â€” SQL hot layer, backlog elevation, agent cloning

**Key architecture evolution â€” SQL as cache, not storage:**
- Brady's insight resolved the original v1 dilemma (filesystem vs. SQL vs. hybrid). SQL is the hot query layer (fast reads within a session), filesystem is the source of truth (durable across sessions). On write: SQL first, then flush to disk. On session start: rehydrate SQL from disk. This eliminates sync risk â€” filesystem always wins, SQL is just a cache that rebuilds itself.
- Kujan's platform assessment confirmed the constraints that make this the right call: SQL is session-scoped, agents can't read coordinator SQL, coordinator blocks on read_agent.

**Team backlog as first-class feature:**
- Brady called the backlog "amazeballs" and the "favorite part." Elevated from proposed concept to primary feature. It's the third memory channel: decisions (agreements) + history (learnings) + backlog (intent). Intent is what makes the team feel predictive.
- Auto-populated from conversation extraction, explicit adds supported, proactive surfacing after agent work.

**Agent cloning â€” already possible, just unused:**
- The drop-box pattern + worktree support + task tool isolation means the same agent identity can spawn multiple times concurrently with no architecture changes. The only blocker is the coordinator's "1-2 agents" guidance. Relaxing that for parallelizable backlog items enables horizontal scaling.
- Key risk is file conflicts â€” mitigated by coordinator assigning non-overlapping scopes and worktree isolation.

**Design principle learned â€” let user feedback evolve alternatives into architecture:**
- v1 proposed three options (filesystem, SQL, hybrid) and recommended filesystem-only. Brady's feedback didn't pick an option â€” he synthesized a new one (SQL as cache layer). The best proposals leave room for the user to improve the architecture. Present options, don't over-commit to one.

### 2026-02-08: v0.1.0 Postmortem â€” State Leak Incident

- **v0.1.0 postmortem:** `.ai-team/` (63 files) leaked to public GitHub repo via tracked git files when dev merged to main for release. Internal planning docs (proposals, blog drafts, demo scripts) also exposed. npm consumers were unaffected due to `package.json` `files` allowlist.
- **Three-layer protection model:** `.gitignore` (prevents tracking) + `package.json` `files` allowlist (prevents npm distribution) + `.npmignore` (explicit exclusion). Belt, suspenders, and a backup belt.
- **Main branch hygiene:** Only product files on main, never planning/team state. 24 files total. Release branch gates what reaches main.
- **The merge gotcha:** When you `git rm --cached` files and merge that change into a branch still tracking them, git deletes the files from disk. Recovery requires restoring from a known-good commit and then unstaging. Always know your restore point.
- **Release branch strategy:** `dev â†’ release â†’ main`. Release branch is a staging gate. Nothing reaches main without passing through release first.
- **Multi-agent state management insight:** Every multi-agent framework that stores state in the filesystem will eventually leak that state. Runtime-generated team state looks like source code but isn't. State hygiene must be built into the product, not left to the user. Postmortem logged to `.ai-team/log/2026-02-08-v0.1.0-postmortem.md`.


### 2026-02-08: Per-Agent Model Selection Design

- **Per-agent model selection:** Charter-level ## Model field (Preferred + Rationale) allows each agent to declare its model needs. Resolution order: user override â†’ charter â†’ registry â†’ auto-selection algorithm. Auto-selection maps role categories to model tiers deterministically.
- **Available models in Copilot CLI:** Opus (premium/vision), Sonnet (standard), Haiku (fast/cheap), GPT-5.x (cross-vendor), Gemini (Google). 16 total models available via the 	ask tool's model parameter.
- **Brady's directive:** Model must match agent capabilities â€” designer needs vision model, tester needs fast model, scribe needs cheapest model. "We don't want Redfoot using Claude Sonnet to design imagery."
- **Delegation support:** Model preference is self-declared in the charter and travels with the agent. Agent-to-agent spawns (delegation) read the target's charter ## Model field â€” same preference regardless of who initiates the spawn. Full auto-selection is coordinator-only; delegating agents use simplified charter-first resolution.
- **Hard dependency:** Auto-selection must ship with or before charter model field. Without it, users must manually configure every agent â€” breaks the zero-config promise.

### 2026-02-09: PR #2 Prompt Review â€” GitHub Issues, PRD Mode, Human Members

- **PR #2 prompt review:** Evaluated @spboyer's three new features (GitHub Issues Mode, PRD Mode, Human Team Members) from a prompt engineering perspective. +316 lines to coordinator prompt (~13.2K â†’ ~17.5K tokens). Context window impact is manageable but this is the inflection point where modular prompt loading should be considered for future additions.
- **Context window budget:** Coordinator at ~17.5K tokens uses ~14% of 128K context. Working context during complex spawns (prompt + team.md + routing.md + registry + inline charters) could hit 35-40K. Instruction priority decay is the risk â€” new sections appended at the end of the prompt are most vulnerable to attention degradation.
- **Pattern gaps identified:** New sections don't adopt several established coordinator patterns: (1) RESPONSE ORDER silent success workaround missing from issue agent spawn prompts, (2) Scribe spawn not referenced after issue work lifecycle, (3) orchestration logging not mentioned for issue/PRD flows, (4) "Feels Heard" acknowledgment pattern not reinforced in issue routing, (5) ceremony integration not addressed for multi-issue fan-out.
- **Parallel execution trap:** GitHub Issues Mode's git branching (`git checkout -b squad/{N}-{slug}`) creates hidden serialization â€” concurrent agents can't safely checkout different branches in the same worktree. Multi-issue parallel work needs explicit worktree guidance or one-at-a-time acknowledgment.
- **PRD decomposition consistency:** Lead spawn prompt for decomposition lacks granularity guidance (how big is one WI?), explicit priority scheme, and splitting heuristics. Will produce inconsistent results across invocations.
- **Human member UX gap:** "Pause and wait" pattern doesn't explicitly state that non-dependent work should continue. Coordinator might over-serialize when a human block affects only one dependency chain.
- **Model selection interaction (Proposal 024):** All PR #2 scenarios work with the existing design. PRD decomposition should trigger the task complexity bump (architecture signal â†’ Lead bumps from Sonnet to Opus). Issue work uses standard charter-based model resolution. No changes to Proposal 024 needed.
- **What's strong:** Branch naming convention (`squad/{N}-{slug}`), PRD approval gate before routing, human "pause and present" pattern, additive Init Mode integration, comparison table for humans vs. AI, 27 prompt validation tests, zero CLI changes.
- **File path:** `.ai-team/agents/verbal/pr2-prompt-review.md`
ðŸ“Œ Team update (2026-02-09): If ask_user returns < 10 characters, treat as ambiguous and re-confirm â€” platform may fabricate default responses from blank input. â€” decided by Brady
ðŸ“Œ Team update (2026-02-09): PR #2 integrated â€” GitHub Issues Mode, PRD Mode, Human Team Members added to coordinator with review fixes (gh CLI detection, post-setup questions, worktree guidance). â€” decided by Fenster
ðŸ“Œ Team update (2026-02-09): Documentation structure formalized â€” docs/ is user-facing only, team-docs/ for internal, .ai-team/ is runtime state. Three-tier separation is permanent. â€” decided by Kobayashi
